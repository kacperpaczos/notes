# ðŸ§  Module 2: Deep Learning

## Cel

Opanowanie koncepcji gÅ‚Ä™bokiego uczenia siÄ™, architektur sieci neuronowych oraz technik treningu i optymalizacji modeli DL.

## Problem

Zrozumienie jak dziaÅ‚ajÄ… gÅ‚Ä™bokie sieci neuronowe, jak je trenowaÄ‡ oraz jak optymalizowaÄ‡ ich wydajnoÅ›Ä‡.

## PojÄ™cia kluczowe

### 1. Architektury sieci neuronowych

* **Perceptron** â€“ podstawowa jednostka sieci neuronowej
* **Sieci wielowarstwowe (MLP)** â€“ Multi-Layer Perceptron
* **Sieci konwolucyjne (CNN)** â€“ Convolutional Neural Networks
* **Sieci rekurencyjne (RNN)** â€“ Recurrent Neural Networks
* **Transformery** â€“ nowoczesne architektury dla NLP

### 2. Funkcje aktywacji

* **ReLU** â€“ Rectified Linear Unit
* **Sigmoid** â€“ dla klasyfikacji binarnej
* **Softmax** â€“ dla klasyfikacji wieloklasowej
* **Tanh** â€“ hiperboliczny tangens

### 3. Techniki treningu

* **Backpropagation** â€“ algorytm wstecznej propagacji bÅ‚Ä™du
* **Gradient descent** â€“ optymalizacja gradientowa
* **Batch, Mini-batch, Stochastic GD** â€“ rÃ³Å¼ne podejÅ›cia do treningu
* **Regularization** â€“ zapobieganie przeuczeniu

## Struktura / Diagram

```
Deep Learning Architecture
â”œâ”€â”€ Input Layer
â”œâ”€â”€ Hidden Layers
â”‚   â”œâ”€â”€ Dense Layers (MLP)
â”‚   â”œâ”€â”€ Convolutional Layers (CNN)
â”‚   â””â”€â”€ Recurrent Layers (RNN/LSTM)
â”œâ”€â”€ Output Layer
â”‚
â”œâ”€â”€ Training Process:
â”‚   â”œâ”€â”€ Forward Pass
â”‚   â”œâ”€â”€ Loss Calculation
â”‚   â”œâ”€â”€ Backward Pass (Backprop)
â”‚   â””â”€â”€ Parameter Update (GD)
â”‚
â””â”€â”€ Optimization:
    â”œâ”€â”€ Adam, RMSProp, SGD
    â”œâ”€â”€ Learning Rate Scheduling
    â””â”€â”€ Regularization (Dropout, L2)
```

## PrzepÅ‚yw dziaÅ‚ania

1. **Przygotowanie danych** â€“ normalizacja, augmentacja
2. **Definicja architektury** â€“ wybÃ³r typu sieci i warstw
3. **Kompilacja modelu** â€“ wybÃ³r optymalizatora, funkcji straty
4. **Trening** â€“ iteracyjny proces uczenia
5. **Ewaluacja** â€“ ocena wydajnoÅ›ci modelu

## PrzykÅ‚ady uÅ¼ycia

* **Computer Vision** â€“ CNN dla rozpoznawania obrazÃ³w
* **NLP** â€“ RNN/LSTM dla przetwarzania jÄ™zyka
* **Generowanie treÅ›ci** â€“ GAN dla generatywnych zadaÅ„
* **Predykcje szeregÃ³w czasowych** â€“ RNN dla prognoz

## Kluczowe przesÅ‚anie

* DL wymaga duÅ¼ych iloÅ›ci danych i mocy obliczeniowej
* WybÃ³r architektury zaleÅ¼y od typu problemu
* Regularization jest kluczowa dla unikniÄ™cia przeuczenia
* Transfer learning przyspiesza rozwÃ³j modeli

## NastÄ™pne kroki

PrzejdÅº do [Module 3: Generative AI](../module-03-generative-ai/) aby poznaÄ‡ techniki generatywnego AI.

## MateriaÅ‚y dodatkowe

- [Deep Learning Book](https://www.deeplearningbook.org/) - Ian Goodfellow
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) - Michael Nielsen
- Dokumentacja TensorFlow/PyTorch
